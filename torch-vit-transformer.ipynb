{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install torchsummary\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2022-12-02T21:20:06.135257Z","iopub.execute_input":"2022-12-02T21:20:06.135683Z","iopub.status.idle":"2022-12-02T21:20:18.914269Z","shell.execute_reply.started":"2022-12-02T21:20:06.135646Z","shell.execute_reply":"2022-12-02T21:20:18.913090Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import ViTFeatureExtractor, ViTModel\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state","metadata":{"execution":{"iopub.status.busy":"2022-12-02T21:21:26.718351Z","iopub.execute_input":"2022-12-02T21:21:26.719280Z","iopub.status.idle":"2022-12-02T21:21:44.760624Z","shell.execute_reply.started":"2022-12-02T21:21:26.719242Z","shell.execute_reply":"2022-12-02T21:21:44.759354Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af6b566ca9b44798839a0910da47c0e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e29a26e912744f788adebb52c44f6ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/330M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efad8622db394e45b955205861994e5a"}},"metadata":{}}]},{"cell_type":"code","source":"# Set up device and training hyperparameters\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T21:21:44.763227Z","iopub.execute_input":"2022-12-02T21:21:44.763638Z","iopub.status.idle":"2022-12-02T21:21:44.775995Z","shell.execute_reply.started":"2022-12-02T21:21:44.763599Z","shell.execute_reply":"2022-12-02T21:21:44.774964Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"epochs = 2\nbatch_size = 512 #Smaller batch - better accuracy\nimage_height = 224\nimage_width = 224\nnum_channels = 3\nlatent_dim = 256\nlearning_rate = 0.001\n\ntrain_dir = \"/kaggle/input/fruits/fruits-360_dataset/fruits-360/Training\"\ntest_dir = \"/kaggle/input/fruits/fruits-360_dataset/fruits-360/Test\"\n\ntransforms_data = transforms.Compose([transforms.Resize((image_height, image_width)),\n                                        transforms.ToTensor(),\n                                     transforms.Normalize([0.6840, 0.5786, 0.5037], [0.3015, 0.3581, 0.3895])])\n\ntrain_dataset = ImageFolder(train_dir, transform=transforms_data)\ntest_dataset = ImageFolder(test_dir, transform=transforms_data)\nnum_classes = len(train_dataset.classes)\n\ndataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\ndataloader_test = DataLoader(test_dataset, batch_size=batch_size * 2, shuffle=False, pin_memory=True)\n\nprint('Train Dataset Size: ', len(train_dataset))\nprint('Test Dataset Size: ', len(test_dataset))","metadata":{"execution":{"iopub.status.busy":"2022-12-02T21:23:09.614051Z","iopub.execute_input":"2022-12-02T21:23:09.614433Z","iopub.status.idle":"2022-12-02T21:23:10.078140Z","shell.execute_reply.started":"2022-12-02T21:23:09.614402Z","shell.execute_reply":"2022-12-02T21:23:10.076150Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Train Dataset Size:  67692\nTest Dataset Size:  22688\n","output_type":"stream"}]},{"cell_type":"code","source":"class CategoryClass(nn.Module):\n    def __init__(self, vit, latent_dim, classes_):\n        super(CategoryClass, self).__init__()\n        \n        self.classes_ = classes_\n        \n        # Set up model architecture\n        self.vit = vit\n        self.fc_1 = nn.Linear(768, latent_dim)\n        self.fc_out = nn.Linear(latent_dim, self.classes_)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, in_data):\n        # Type of output `BaseModelOutputWithPooling`\n        vit_outputs = self.vit(in_data)\n        \n        # Shape of pooler_output: (batch_size, hidden_size)\n        pooler_output = vit_outputs.pooler_output\n        \n        # Pass through the linear layout to predict the class\n        # Shape of output: (batch_size, classes_)\n        outputs = torch.relu(self.fc_1(pooler_output))\n        outputs = self.fc_out(self.dropout(outputs))\n        \n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-12-02T21:23:10.079982Z","iopub.execute_input":"2022-12-02T21:23:10.080263Z","iopub.status.idle":"2022-12-02T21:23:10.087665Z","shell.execute_reply.started":"2022-12-02T21:23:10.080236Z","shell.execute_reply":"2022-12-02T21:23:10.086703Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')","metadata":{"execution":{"iopub.status.busy":"2022-12-02T21:23:10.256300Z","iopub.execute_input":"2022-12-02T21:23:10.256969Z","iopub.status.idle":"2022-12-02T21:23:11.738318Z","shell.execute_reply.started":"2022-12-02T21:23:10.256927Z","shell.execute_reply":"2022-12-02T21:23:11.737190Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Freeze the layers in vit exclude the pooler layers\nfor param in vit.parameters():\n    param.requires_grad = False\n\nvit.pooler.dense.weight.requires_grad = True\nvit.pooler.dense.bias.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2022-12-02T21:23:11.740727Z","iopub.execute_input":"2022-12-02T21:23:11.741184Z","iopub.status.idle":"2022-12-02T21:23:11.748222Z","shell.execute_reply.started":"2022-12-02T21:23:11.741142Z","shell.execute_reply":"2022-12-02T21:23:11.746907Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model = CategoryClass(vit,\n                      latent_dim,\n                      len(train_dataset.classes)).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T21:23:11.750029Z","iopub.execute_input":"2022-12-02T21:23:11.750483Z","iopub.status.idle":"2022-12-02T21:23:15.038932Z","shell.execute_reply.started":"2022-12-02T21:23:11.750443Z","shell.execute_reply":"2022-12-02T21:23:15.037879Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), learning_rate)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T21:23:15.041764Z","iopub.execute_input":"2022-12-02T21:23:15.042259Z","iopub.status.idle":"2022-12-02T21:23:15.049552Z","shell.execute_reply.started":"2022-12-02T21:23:15.042217Z","shell.execute_reply":"2022-12-02T21:23:15.048093Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\nimport math\n# Save the training and validating information\ntraining_loss_history =[]\ntraining_accuracy_history = []\nvalidating_loss_history = []\nvalidating_accuracy_history = []\n\n# We only train one epoch\n#  Therefore, we save the batch information\nbatch_loss_history = []\nbatch_accuracy_history = []\n\nprint('Start Training')\nprint('*'*100)\nstart_time = datetime.now()\nfor epoch in range(epochs):\n    # Set to the train mode\n    model.train()\n    \n    train_epoch_loss = 0.0\n    train_epoch_accuracy = 0.0\n    for idx, (images, labels) in enumerate(dataloader_train):\n        # Move the data to the device\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Pass through the model\n        outputs = model(images)\n        \n        # Count the loss and update the parameters\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Record the training information\n        train_epoch_loss += loss.item()\n        predict_class = outputs.argmax(dim=-1)\n        accuracy = torch.sum(predict_class == labels).item() / labels.shape[0]\n        train_epoch_accuracy += accuracy\n\n        # Save the batch information\n        batch_loss_history.append(loss.item())\n        batch_accuracy_history.append(accuracy)\n\n        # Print batch information\n    #print(f\" in epoch {epoch + 1}/{epochs}\")\n        #print(f\"Average loss: {loss.item()}\")\n        #print(f\"Average accuracy {accuracy}\")\n        \n    # Set to the eval mode\n    model.eval()\n    \n    val_epoch_loss = 0.0\n    val_epoch_accuracy = 0.0\n    \n    with torch.no_grad():\n        for val_images, val_labels in dataloader_test:\n            # Move data to the device\n            val_images = val_images.to(device)\n            val_labels = val_labels.to(device)\n            \n            # Pass through model\n            val_outputs = model(val_images)\n            \n            # Count the loss and accuracy\n            val_epoch_loss += criterion(val_outputs, val_labels)\n            val_predict_class = val_outputs.argmax(dim=-1)\n            val_epoch_accuracy += torch.sum(val_predict_class == val_labels).item() / val_labels.shape[0]\n            \n    # Save the epoch information\n    training_loss_history.append(100*train_epoch_loss / len(dataloader_train))\n    training_accuracy_history.append(100*train_epoch_accuracy / len(dataloader_train))\n    validating_loss_history.append(100*val_epoch_loss / len(dataloader_test))\n    validating_accuracy_history.append(100*val_epoch_accuracy / len(dataloader_test))\n    \n    print('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}, Testing Loss: {:.3f}, Testing Acc: {:.3f}'\n          .format(epoch + 1, epochs, training_loss_history[-1], training_accuracy_history[-1], validating_loss_history[-1], validating_accuracy_history[-1]))\n\n    end_time = datetime.now()\n    epoch_time = (end_time - start_time).total_seconds()\n    print(\"-\"*100)\n    print('Epoch Time : ', math.floor(epoch_time // 60), ':', math.floor(epoch_time % 60))\n    print(\"-\"*100)\n    \nprint(\"*** Training Completed ***\")","metadata":{"execution":{"iopub.status.busy":"2022-12-02T21:23:15.051417Z","iopub.execute_input":"2022-12-02T21:23:15.052233Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Start Training\n****************************************************************************************************\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model weight\ntorch.save(model.state_dict(), \"model.pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the loss and accuracy\nplt.figure(figsize=(18,6))\n\n# Loss\nplt.subplot(1, 2, 1)\nplt.title(\"Loss for batch\")\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\nplt.plot(batch_loss_history)\n\n# Accuracy\nplt.subplot(1, 2, 2)\nplt.title(\"Accuracy for batch\")\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Accuracy\")\nplt.plot(batch_accuracy_history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
